{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01dd8f2e-4e74-4ff3-bce4-5468703b0807",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "stage_table_name = \"incremental_load.default.orders_stage\"\n",
    "target_table_name = \"incremental_load.default.orders_target\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41784ce9-8d58-4e7a-a8ac-f872a7dde12d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the data from the stage table\n",
    "stage_df = spark.read.table(stage_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee48b5c8-db72-4dd9-be70-412fddeab42c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# if not spark._jsparkSession.catalog().tableExists(target_table_name):\n",
    "#     stage_df.write.format(\"delta\").saveAsTable(target_table_name)\n",
    "    \n",
    "# else:\n",
    "#     # Perform delta table merge query for upsert based on tracking_num column\n",
    "#     target_table = DeltaTable.forName(spark, target_table_name)\n",
    "\n",
    "#     # Define the merge condition based on the tracking_num column\n",
    "#     merge_condition = \"stage.tracking_num = target.tracking_num\"\n",
    "\n",
    "#     # Execute the merge operation\n",
    "#     target_table.alias(\"target\") \\\n",
    "#         .merge(stage_df.alias(\"stage\"), merge_condition) \\\n",
    "#         .whenMatchedDelete() \\\n",
    "#         .execute()\n",
    "\n",
    "#     stage_df.write.format(\"delta\").mode(\"append\").saveAsTable(target_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9004aaed-16e9-41a4-8deb-a5fb7007a09a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "\n",
    "# Helper: Check if a table exists\n",
    "def table_exists(table_name: str) -> bool:\n",
    "    try:\n",
    "        spark.table(table_name)\n",
    "        return True\n",
    "    except AnalysisException:\n",
    "        return False\n",
    "\n",
    "# Helper: Check if a table is a Delta table\n",
    "def is_delta_table(table_name: str) -> bool:\n",
    "    try:\n",
    "        return DeltaTable.isDeltaTable(spark, table_name)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Main logic\n",
    "if not table_exists(target_table_name):\n",
    "    print(\"Target table does not exist. Creating as Delta table...\")\n",
    "    stage_df.write.format(\"delta\").saveAsTable(target_table_name)\n",
    "\n",
    "elif not is_delta_table(target_table_name):\n",
    "    print(\"Target table exists but is NOT a Delta table. Dropping and recreating...\")\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {target_table_name}\")\n",
    "    stage_df.write.format(\"delta\").saveAsTable(target_table_name)\n",
    "\n",
    "else:\n",
    "    print(\"Target table exists and is a Delta table. Performing SCD1 merge...\")\n",
    "    target_table = DeltaTable.forName(spark, target_table_name)\n",
    "\n",
    "    merge_condition = \"stage.tracking_num = target.tracking_num\"\n",
    "\n",
    "    target_table.alias(\"target\") \\\n",
    "        .merge(\n",
    "            stage_df.alias(\"stage\"),\n",
    "            merge_condition\n",
    "        ) \\\n",
    "        .whenMatchedUpdateAll() \\\n",
    "        .whenNotMatchedInsertAll() \\\n",
    "        .execute()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "orders_target_load",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
