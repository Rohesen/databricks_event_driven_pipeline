# ðŸ¥« Order Tracking - Event Driven Data Ingestion

## ðŸ“Œ Project Overview
This project implements an **event-driven data ingestion pipeline** in **Databricks** for processing order tracking files.  
It automatically triggers workflows upon **file arrival**, stages the incoming data, and performs **SCD Type 1 (upsert) merges** into a Delta Lake target table.

The workflow consists of two jobs:
1. **Orders Stage Load** - Reads raw files from Databricks Volumes and loads them into a staging Delta table.
2. **Orders Target Merge** - Performs an incremental upsert (SCD1) from the staging table into the target Delta table.

---

## ðŸ›  Tech Stack
- **Google Cloud Storage (GCS)** â€“ Raw file storage
- **Databricks** â€“ Data engineering & orchestration
- **PySpark** â€“ Distributed data processing
- **Delta Lake** â€“ ACID-compliant storage format
- **Databricks Workflows** â€“ Job scheduling & triggers
- **GitHub** â€“ Version control for notebooks

---

## ðŸ“‚ Project Structure
```plaintext
.
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ orders_stage.py       # Stage job: Load raw files â†’ staging Delta table
â”‚   â”œâ”€â”€ orders_target.py      # Target job: Merge from staging â†’ target Delta table
â”œâ”€â”€ README.md
